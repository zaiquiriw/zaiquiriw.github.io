{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Howdy There!", "text": "<p>This website hosts my work from my ML experience at UTD. I learned to create primarily predictive models on structured data utilizing R and Python to implement linear regression to deep learning.</p> <p>My academic journey in Computer Science was transformative, but it was my focus on machine learning that truly shaped my passion and expertise. The hands-on experience I gained has fueled my desire to continue learning and growing in the field of data science.</p> <p>Influenced by my professor, who transitioned from industry to academia, I too am considering a similar path. I believe that real-world experience can provide invaluable insights that can be brought back into academic or research settings.</p> <p>Feel free to explore my projects and reach out if you have any questions or opportunities that align with my interests.</p>"}, {"location": "#tldr", "title": "TLDR", "text": "<p>If you want the quick version of my skills, just look over my short and sweet resume.</p>"}, {"location": "#my-projects", "title": "My Projects", "text": "<p>Out of my different projects, I\u2019m most proud of the work I did researching explainability in attention based models and fine tuning Google\u2019s MobileNetV2 for identifying grains of rice.</p> <p></p> <p>You can view all of my introduction to machine learning work as well as my natural language processing work as pdfs here, or look at the source on github.</p> <p>I can\u2019t wait to do more projects and add more experince to this site!</p>"}, {"location": "#keep-up-with-zaiquiri", "title": "Keep up with Zaiquiri", "text": "<p>This month I\u2019ll be committing nanowrimo and begin work on a podcast where I talk through common programming problems and algorithms through the lense of science fiction and AI. I hope to not only improve as a critical thinker but help explain difficult concepts to other people as well. I\u2019ll post my progress here in updates once I get started!</p>"}, {"location": "ML%20Work/", "title": "ML Work", "text": "<p>Back in 2022 I took an introduction to Machine Learning with the wonderful Karen Mazidi who gave us a large overview of both data science basics, and basic machine learning. The class was project based, with a focus on providing documentation of the process.</p>"}, {"location": "ML%20Work/#learning-in-r", "title": "Learning in R", "text": "<p>We covered using R for a variety of algorithms:</p> <ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Naive Bayes</li> <li>kNN</li> <li>k-means Clustering</li> <li>Decision Trees and Random Forests</li> <li>Support Vector Machines</li> </ul> <p>As well as best practices for picking good attributes for analysis, cleaning up data in CSVs, and visualizing the results</p>"}, {"location": "ML%20Work/#learning-in-python", "title": "Learning in Python", "text": "<p>We then pivoted to implementing these solutions in the Python package ecosystem, using:</p> <ul> <li>NumPy</li> <li>Pandas</li> <li>Scikit-Learn</li> <li>Seaborn</li> </ul> <p>To implement all the algorithms we just learned. Python allowed us to branch into Neural Networks using Keras for the implementation of zero-shot classification</p> <p>Note</p> <p>We even touched on Hidden Markov Models and Bayesian nets, but not much on their implementation</p>"}, {"location": "ML%20Work/#continued-work", "title": "Continued Work", "text": "<p>This class carried into my work in NLP, which happened to coincide with the emergence of Open-AI\u2019s ChatGPT. Check it out for work on more advanced neural nets</p>"}, {"location": "ML%20Work/#the-projects", "title": "The Projects", "text": "<p>While I will leave my original code open source on github the following brief summaries will link to pdfs summarizing the projects.</p> <ul> <li>Low Level Basics: I worked a little bit in C++ to implement basic statistics calculations, just to make sure I had the swing of things.</li> <li>Linear Models: Assuming that problem, its input and output, are linearly related, there are multiple ways to create a predictive supervised model:<ul> <li>Linear Regression</li> <li>Linear Classification: Naive Bayes and Logistic Regression</li> <li>Building From Scratch</li> </ul> </li> <li>Similarities: If instead of predicting a target value, we just wanted to understand the data, we have many different methods of breaking down complex (and high dimensional) data. This works hand in hand with dimensionality reduction. I have more on the subject here. kNN and K-means algorithms were both good tools to improve the performance of our previous models.<ul> <li>Using similarties to improve regression</li> <li>Using similarities to improve classification</li> <li>Clustering Spotify genres</li> <li>Failed dimensionality reduction of Spotify</li> </ul> </li> <li>Support Vector Machines: SVM\u2019s divide data in such a way that optimizes the margin between data. While we can perhaps visualize data being split by a line for classification, this method can not only be used for high dimensional classification but regression as well. I have a much better description here.<ul> <li>Classifciaton with SVMs</li> <li>Regression with SVMs</li> </ul> </li> <li>Neural Networks: While I explore neural networks in full in my future Natural Language Processing class, we still got some experience with how neural networks work with keras and tensorflow. Using RNN, CNN, and even finetuning on Google\u2019s MobileNet V2, I was able to create a pretty good rice identification model.</li> </ul> <p>Out of anything I would recommend reading my Keras Image Classification paper for a good picture of the progress made in this class. Needless to say this was maybe the second most impactful class of my degree, right before NLP.</p>"}, {"location": "NLP%20Work/", "title": "NLP Work", "text": "<p>Continuing my study in machine learning, I decided to focus on language processing and take a class on NLP. My class focused on learning the various libraries and ML techniques we use to under stand language, and scaling that up in python all the way to deep learning in python. We covered:</p> <ul> <li>Foundational NLP Language distinctions like Parts of Speech and word, sentance, and corpora</li> <li>Basic Python usage with NLTK for preprocessing</li> <li>Wordnet and building word relationships</li> <li>N-gram models for language generation</li> <li>Context Free Grammars</li> <li>Numpy, pandas, scikit-learn, and seaborn</li> <li>Naive Bayes and Logistic Regression for NLP</li> <li>Keras for CNN\u2019s, RNN\u2019s, LSTM and GRU</li> <li>Using embeddings along with decoders and encoders</li> </ul> <p>For all of these topics we did various projects to get better at implementing our knowledge and sharing it using jupyter notebooks.</p>"}, {"location": "NLP%20Work/#the-projects", "title": "The Projects", "text": "<p>If you would like to view the code and notebook work related to these projects they are still posted on github to view! However here are some short summaries of my work in NLP. I value my analysis of attention as an explainability metric if you would like to view it!</p> <ul> <li>Wordnets: This is an exploration of how wordnets can reveal complex meanings of words not simply found in the definition</li> <li>N-grams: Just a brief description of ngrams to illustrate their usefulness</li> <li>Netscraping for LLM\u2019s: I used BeautifulSoup to scrape the web for an LLM</li> <li>text-classification.pdf: I used simple Neural Networks with the goal of building a network that could be used to train a network on imitating characters (in this case Rick and Morty\u2019s voice and tone)</li> <li>The Impact of Attention: This short paper summarizes a paper on the impact of a \u201cIs Attention Explanation\u201d and bridges the creation of modern GPTs into the now pressing Alignment problem and other consequences of modern attention. A personal favorite project where I explored the quakes in AI research sudden prominence of new AI techniques.</li> <li>More Rick And Morty: I liked to have fun, so I did a take two on classifying text based on the Rick and Morty voice. However, it came out more on a study on how you can\u2019t squeeze data to work your use case. You just have to work with the data you have. </li> </ul> <p>I came out of this class really wanting to do more research, but I did not want to jump right into a masters. Perhaps one day, but I need a break after 16 or so years of schooling. I do feel very comfortable in data science, and I value that greatly!</p>"}, {"location": "nlp%20overview/", "title": "Nlp overview", "text": ""}, {"location": "nlp%20overview/#natural-language-processing", "title": "Natural Language Processing", "text": "<p>Continuing my study in machine learning, I decided to focus on language processing and take a class on NLP. My class focused on learning the various libraries and ML techniques we use to under stand language, and scaling that up in python all the way to deep learning in python. We covered:</p> <ul> <li>Foundational NLP Language distinctions like Parts of Speech and word, sentance, and corpora</li> <li>Basic Python usage with NLTK for preprocessing</li> <li>Wordnet and building word relationships</li> <li>N-gram models for language generation</li> <li>Context Free Grammars</li> <li>Numpy, pandas, scikit-learn, and seaborn</li> <li>Naive Bayes and Logistic Regression for NLP</li> <li>Keras for CNN\u2019s, RNN\u2019s, LSTM and GRU</li> <li>Using embeddings along with decoders and encoders</li> </ul> <p>For all of these topics we did various projects to get better at implementing our knowledge and sharing it using jupyter notebooks.</p>"}, {"location": "nlp%20overview/#the-projects", "title": "The Projects", "text": "<p>If you would like to view the code and notebook work related to these projects they are still posted on github to view! However here are some short summaries of my work in NLP. I value my analysis of attention as an explainability metric if you would like to view it!</p> <ul> <li>Wordnets: This is an exploration of how wordnets can reveal complex meanings of words not simply found in the definition</li> <li>N-grams: Just a brief description of ngrams to illustrate their usefulness</li> <li>Netscraping for LLM\u2019s: I used BeautifulSoup to scrape the web for an LLM</li> <li>text-classification.pdf: I used simple Neural Networks with the goal of building a network that could be used to train a network on imitating characters (in this case Rick and Morty\u2019s voice and tone)</li> <li>The Impact of Attention: This short paper summarizes a paper on the impact of a \u201cIs Attention Explanation\u201d and bridges the creation of modern GPTs into the now pressing Alignment problem and other consequences of modern attention. A personal favorite project where I explored the quakes in AI research sudden prominence of new AI techniques.</li> <li>More Rick And Morty: I liked to have fun, so I did a take two on classifying text based on the Rick and Morty voice. However, it came out more on a study on how you can\u2019t squeeze data to work your use case. You just have to work with the data you have. </li> </ul> <p>I came out of this class really wanting to do more research, but I did not want to jump right into a masters. Perhaps one day, but I need a break after 16 or so years of schooling. I do feel very comfortable in data science, and I value that greatly!</p>"}, {"location": "ML%20Work/data%20exploration/", "title": "Data exploration", "text": ""}, {"location": "ML%20Work/data%20exploration/#data-exploration", "title": "Data Exploration", "text": ""}, {"location": "ML%20Work/data%20exploration/#the-premise", "title": "The Premise", "text": "<p>\u201cIn class, we covered how to do data exploration with statistical functions in R. In this assignment, you recreate that functionality in C++ code. This will prepare us to write algorithms in C++ in future assignments\u201d</p> <p>For me this is both a review of C++, but also a review of what correlation is.</p>"}, {"location": "ML%20Work/data%20exploration/#notes", "title": "Notes", "text": "<ul> <li>I deliberate whether returning range as the min and max, or as the difference between the two. I eventually chose just returning a min and max.</li> <li>I can\u2019t get relative links to work at the moment, hope it\u2019s fine that it is linking to the file hosted on the main site</li> <li></li> </ul>"}, {"location": "ML%20Work/data%20exploration/#conclusion", "title": "Conclusion", "text": ""}, {"location": "ML%20Work/data%20exploration/#the-code", "title": "The Code", "text": "<pre><code>#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n#include &lt;fstream&gt;\n#include &lt;algorithm&gt;\n#include &lt;iterator&gt;\n#include &lt;cmath&gt;\n\nusing namespace std;\n\n// TODO: Convert double vectors to taking in (explicitly) any numeric value\n// Reference: - Iterators: https://www.geeksforgeeks.org/iterators-c-stl/\n//            - What get's passed into sort: https://cplusplus.com/reference/iterator/RandomAccessIterator/\n\nclass Explore\n{\npublic:\n    // Calculate the sum of the vector\n    double sum_vector(vector&lt;double&gt; vect)\n    {\n        double sum = 0;\n        for (int i = 0; i &lt; vect.size(); i++)\n        {\n            sum += vect[i];\n        }\n        return sum;\n    }\n\n    // Calculate the mean of a vector\n    double mean_vector(vector&lt;double&gt; vect)\n    {\n        double mean = sum_vector(vect) / vect.size();\n        return mean;\n    }\n\n    // Calculate the median of a vector\n    double median_vector(vector&lt;double&gt; vect)\n    {\n        double median;\n        // Use an iterator because it is probably better -internet\n        vector&lt;double&gt;::iterator it;\n        // Find the center if it is even or odd\n        sort(vect.begin(), vect.end());\n        if (vect.size() % 2 == 0) // If there is an even number of elements\n        {\n            it = vect.begin() + vect.size() / 2 - 1;\n            median = (*it + *(it + 1)) / 2;\n        }\n        else // if there is an odd number of elements\n        {\n            it = vect.begin() + vect.size() / 2;\n            median = *it;\n        }\n        return median;\n    }\n\n    // Calculate the range of a vector\n    vector&lt;double&gt; range_vector(vector&lt;double&gt; vect)\n    {\n        vector&lt;double&gt; range = {max_vector(vect), min_vector(vect)};\n        return range;\n    }\n\n    // Calculate the max of a vector (Just for range)\n    double max_vector(vector&lt;double&gt; vect)\n    {\n        double max;\n        vector&lt;double&gt;::iterator it;\n        sort(vect.begin(), vect.end());\n        it = vect.end() - 1;\n        max = *it;\n        return max;\n    }\n\n    // Calculate the min of a vector (Just for range) just with a loop\n    double min_vector(vector&lt;double&gt; vect)\n    {\n        double min;\n        vector&lt;double&gt;::iterator it;\n        sort(vect.begin(), vect.end());\n        it = vect.begin();\n        min = *it;\n        return min;\n    }\n\n    // Calculate the covariance of two vectors\n    // Cov(x,y) = E((x-x_mean)(y-y_mean)))/n-1\n    double covar_vector(vector&lt;double&gt; x, vector&lt;double&gt; y)\n    {\n        double sum = 0;\n        double mean_x = mean_vector(x);\n        double mean_y = mean_vector(y);\n        for (int i = 0; i &lt; x.size(); i++)\n        {\n            float x_i_diff = x[i] - mean_x;\n            float y_i_diff = y[i] - mean_y;\n            float y_times_x_diff = x_i_diff * y_i_diff;\n            // cout &lt;&lt; x_i_diff &lt;&lt; \" * \" &lt;&lt; y_i_diff &lt;&lt; \" = \" &lt;&lt; y_times_x_diff &lt;&lt; endl;\n            sum = sum + y_times_x_diff;\n        }\n        return sum / (x.size() - 1);\n    }\n\n    // Calculate the correlation of two vectors\n    // Cor(x,y) = Cov(x,y)/(standard_devation(x)*standard_devation(y))\n    // Using the hint from the assignment:\n    // \"sigma of a vector can be calculated as the square root of variance(v,v)\"\n    double cor_vector(vector&lt;double&gt; x, vector&lt;double&gt; y)\n    {\n        double covar = covar_vector(x, y);\n        double sigma_x = sqrt(covar_vector(x, x));\n        double sigma_y = sqrt(covar_vector(y, y));\n        return covar / (sigma_x * sigma_y);\n    }\n\n    // Run suite of statistcal functions on a vector\n    void print_stats(vector&lt;double&gt; vect)\n    {\n        cout &lt;&lt; \"Sum:    \" &lt;&lt; sum_vector(vect) &lt;&lt; endl;\n        cout &lt;&lt; \"Mean:   \" &lt;&lt; mean_vector(vect) &lt;&lt; endl;\n        cout &lt;&lt; \"Median: \" &lt;&lt; median_vector(vect) &lt;&lt; endl;\n        vector&lt;double&gt; range = range_vector(vect);\n        cout &lt;&lt; \"Range:  \" &lt;&lt; range[1] &lt;&lt; \", \" &lt;&lt; range[0] &lt;&lt; endl;\n    }\n};\n\nint main(int argc, char **argv)\n{\n    ifstream inFS;\n    string line;\n    string rm_in, medv_in;\n    const int MAX_LEN = 1000;\n    vector&lt;double&gt; rm(MAX_LEN), medv(MAX_LEN);\n\n    cout &lt;&lt; \"Opening file Boston.csv.\" &lt;&lt; endl;\n\n    inFS.open(\"Boston.csv\");\n    if (!inFS.is_open())\n    {\n        cout &lt;&lt; \"Error opening file Boston.csv.\" &lt;&lt; endl;\n        return 1;\n    }\n\n    cout &lt;&lt; \"Reading line 1 of Boston.csv.\" &lt;&lt; endl;\n    getline(inFS, line);\n\n    // echo heading\n    cout &lt;&lt; \"Headings: \" &lt;&lt; line &lt;&lt; endl;\n\n    // read data\n    int numObservations = 0;\n    while (inFS.good())\n    {\n        getline(inFS, rm_in, ',');\n        getline(inFS, medv_in, '\\n');\n        rm.at(numObservations) = stof(rm_in);\n        medv.at(numObservations) = stof(medv_in);\n\n        numObservations++;\n    }\n\n    rm.resize(numObservations);\n    medv.resize(numObservations);\n\n    cout &lt;&lt; \"New Length: \" &lt;&lt; rm.size() &lt;&lt; endl;\n\n    cout &lt;&lt; \"Closing file Boston.csv.\" &lt;&lt; endl;\n    inFS.close(); // Done\n\n    cout &lt;&lt; \"Number of records: \" &lt;&lt; numObservations &lt;&lt; endl;\n\n    // Create an Explore object to use stats functions\n    Explore explore;\n\n    cout &lt;&lt; \"\\nStats for rm\" &lt;&lt; endl;\n    explore.print_stats(rm);\n\n    cout &lt;&lt; \"\\nStats for medv\" &lt;&lt; endl;\n    explore.print_stats(medv);\n\n    cout &lt;&lt; \"\\n Covariance = \" &lt;&lt; explore.covar_vector(rm, medv) &lt;&lt; endl;\n\n    cout &lt;&lt; \"\\n Correlation = \" &lt;&lt; explore.cor_vector(rm, medv) &lt;&lt; endl;\n\n    cout &lt;&lt; \"\\nProgram terminated.\" &lt;&lt; endl;\n}\n</code></pre>"}, {"location": "ML%20Work/data%20exploration/#returns", "title": "Returns", "text": "<pre><code>Opening file Boston.csv.\nReading line 1 of Boston.csv.\nHeadings: rm,medv\nNew Length: 506\nClosing file Boston.csv.\nNumber of records: 506\n\nStats for rm\nSum:    3180.03\nMean:   6.28463\nMedian: 6.2085\nRange:  3.561, 8.78\n\nStats for medv\nSum:    11401.6\nMean:   22.5328\nMedian: 21.2\nRange:  5, 50\n\n Covariance = 4.49345\n\n Correlation = 0.69536\n\nProgram terminated.\n</code></pre>"}, {"location": "ML%20Work/data%20exploration/#built-into-r-or-c", "title": "Built into R or C++", "text": "<p>I believe that it was clearly easier to use R functions vs going through and making these functions in C++. This indicates the value of use using R into the future for machine learning. A more straight forward way to analyze data will allow us to understand our data models.</p> <p>I will note that someone fluent in C++ would do better than I did, considering I was just stumbling around for a bit trying to remember how import a library for a second there.</p>"}, {"location": "ML%20Work/data%20exploration/#statistical-value", "title": "Statistical Value", "text": "<p>What statistical measures did I evaluate:</p> <ul> <li>Mean: A mean is an average of the data set, and represents the typical or most likely value from a dataset. Knowing what values in a dataset tend to be is important in understanding what general trend of all the data in your dataset is. You can compare values to this to find outliers and such.</li> <li>Median: The center value of the data as it is in sorted order. This tells you a central tendency independent of skewed data or large outliers</li> <li>Range: is the minimum and maximum values the values of the dataset might take. It is useful to understand how a dataset is bounded, both to see how far outliers might be from the center of a dataset, and just to get an idea for what the data looks like in scale.</li> </ul> <p>Whenever we are organizing data for a machine learning algorithm, we must understand our data ourselves to have a hope of predicting a trend given our data. Looking at values like mean or median tell us easy to understand generalizations about data so that we may get a general understanding of the meaning of our data without having to analyze every data point. Given more and more powerful generalization tools, or methods of analyzation, we can grow more and more confident in the understanding of our data.</p> <p>If we can see a general trend using our descriptive statistical measurements, then we can assure our model will be able to eventually get the specific trend data we hope to predict from that dataset.</p>"}, {"location": "ML%20Work/data%20exploration/#covariance-and-correlation", "title": "Covariance and Correlation", "text": "<p>Given two attributes that may or may not be related, we may find the covariance and correlation between those two bits of data. The covariance tells how one attributes data might relate to another. If x\u2019s covariance to y is a high positive number, we know that as x goes up y goes down, and vice versa. Correlation is just a version of that number, scaled down to a range of (-1, 1) in order to make the factor much more uniform</p> <p>The values are different from those above, considering they are not just measurements of an attribute of data but an extrapolation from that data about relationships or patterns. This is very useful when working in ml, as our end goal is figuring out how data tends to relate to certain results. Using how data correlates then directly supports our end goal of predicting outcomes, or just understanding complex relationships.</p>"}, {"location": "ML%20Work/ml%20overview/", "title": "ML Work", "text": ""}, {"location": "ML%20Work/ml%20overview/#machine-learning-work", "title": "Machine Learning Work", "text": "<p>Back in 2022 I took an introduction to Machine Learning with the wonderful Karen Mazidi who gave us a large overview of both data science basics, and basic machine learning. The class was project based, with a focus on providing documentation of the process.</p>"}, {"location": "ML%20Work/ml%20overview/#learning-in-r", "title": "Learning in R", "text": "<p>We covered using R for a variety of algorithms:</p> <ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Naive Bayes</li> <li>kNN</li> <li>k-means Clustering</li> <li>Decision Trees and Random Forests</li> <li>Support Vector Machines</li> </ul> <p>As well as best practices for picking good attributes for analysis, cleaning up data in CSVs, and visualizing the results</p>"}, {"location": "ML%20Work/ml%20overview/#learning-in-python", "title": "Learning in Python", "text": "<p>We then pivoted to implementing these solutions in the Python package ecosystem, using:</p> <ul> <li>NumPy</li> <li>Pandas</li> <li>Scikit-Learn</li> <li>Seaborn</li> </ul> <p>To implement all the algorithms we just learned. Python allowed us to branch into Neural Networks using Keras for the implementation of zero-shot classification</p> <p>Note</p> <p>We even touched on Hidden Markov Models and Bayesian nets, but not much on their implementation</p>"}, {"location": "ML%20Work/ml%20overview/#continued-work", "title": "Continued Work", "text": "<p>This class carried into my work in NLP, which happened to coincide with the emergence of Open-AI\u2019s ChatGPT. Check it out for work on more advanced neural nets</p>"}, {"location": "ML%20Work/ml%20overview/#the-projects", "title": "The Projects", "text": "<p>While I will leave my original code open source on github the following brief summaries will link to pdfs summarizing the projects.</p> <ul> <li>Low Level Basics: I worked a little bit in C++ to implement basic statistics calculations, just to make sure I had the swing of things.</li> <li>Linear Models: Assuming that problem, its input and output, are linearly related, there are multiple ways to create a predictive supervised model:<ul> <li>Linear Regression</li> <li>Linear Classification: Naive Bayes and Logistic Regression</li> <li>Building From Scratch</li> </ul> </li> <li>Similarities: If instead of predicting a target value, we just wanted to understand the data, we have many different methods of breaking down complex (and high dimensional) data. This works hand in hand with dimensionality reduction. I have more on the subject here. kNN and K-means algorithms were both good tools to improve the performance of our previous models.<ul> <li>Using similarties to improve regression</li> <li>Using similarities to improve classification</li> <li>Clustering Spotify genres</li> <li>Failed dimensionality reduction of Spotify</li> </ul> </li> <li>Support Vector Machines: SVM\u2019s divide data in such a way that optimizes the margin between data. While we can perhaps visualize data being split by a line for classification, this method can not only be used for high dimensional classification but regression as well. I have a much better description here.<ul> <li>Classifciaton with SVMs</li> <li>Regression with SVMs</li> </ul> </li> <li>Neural Networks: While I explore neural networks in full in my future Natural Language Processing class, we still got some experience with how neural networks work with keras and tensorflow. Using RNN, CNN, and even finetuning on Google\u2019s MobileNet V2, I was able to create a pretty good rice identification model.</li> </ul> <p>Out of anything I would recommend reading my Keras Image Classification paper for a good picture of the progress made in this class. Needless to say this was maybe the second most impactful class of my degree, right before NLP.</p>"}, {"location": "obsidian/copart%20internship/", "title": "Copart internship", "text": ""}, {"location": "obsidian/copart%20internship/#data-science-at-copart", "title": "Data Science at Copart", "text": "<p>The company Copart is right next to my house. Right now, as of 2 days ago, they have a position open for a data scientist internship right now! I meet all of the requirements, and have the portfolio to support it. Now I just need to put all that together for a</p>"}, {"location": "obsidian/copart%20internship/#the-job", "title": "The Job", "text": "<p>Copart is looking for a data scientist who will work closely with IT and various other departments to drive insight into data and deliver machine learning solutions to improve Copart\u2019s operations. This data scientist will also design/manipulate large scale data sets from a multitude of sources, work to operationalize and integrate machine learning solutions into Copart\u2019s current products and visualize and report on findings and results to provide insight to the organization.  </p> <p>Job Duties</p> <ul> <li>Develop new predictive models using advanced techniques</li> <li>Apply critical thinking to ensure data integrity and quality control is applied to each dataset, model and other analysis prior to presenting with internal customers</li> <li>Coordinate with different functional teams to operationalize, and monitor machine learning solutions</li> <li>Apply statistical methodologies such as cluster and regression analysis, if necessary.</li> <li>Act as a proponent of data science/analytics to senior leadership and others by being able to explain the benefits of machine learning, and other techniques.</li> <li>6 Months of experience (relevant academic internships &amp; projects can be considered in lieu of professional experience) with machine learning, statistical modeling, and data mining techniques.</li> <li>Bachelor or Master\u2019s degree in highly quantitative field (computer science, mathematics, machine learning, statistics) or equivalent experience</li> <li>Proficiency in either R or Python</li> <li>Proficiency in data sourcing/manipulation in SQL</li> <li>Bachelor or Master\u2019s degree in highly quantitative field (computer science, mathematics, machine learning, statistics) or equivalent experience</li> <li>Experience applying various machine learning techniques, specifically neural networks and gradient boosted machines, and understanding the key parameters that affect their performance</li> <li>Strong data visualization skills using open source tools (plotly, ggplot2, shiny)</li> <li>Experience with both supervised and unsupervised modeling techniques</li> </ul> <p>This is, simply, exactly what I have experience in. But I haven\u2019t done anything in the field recently.</p>"}, {"location": "obsidian/copart%20internship/#what-i-want-to-do", "title": "What I Want to Do", "text": "<p>I am terrified of applying to jobs. So I am going to compromise between not applying and applying right now. I\u2019d say I have to prep my past work on the subject</p> <ul> <li> Combine my work from Mazidi\u2019s NLP and ML classes into one portfolio in markdown</li> <li> Host it on my URL with a home page summary that links to my linkedin and github</li> <li> Clean up my github</li> <li> Clean up my Resume to be focused on Data Science</li> <li> Review focus points on the list of requirements:<ul> <li> Python basic programming questions</li> <li> Profeciency with ggplot2 and pandas, matplot, pytorch, and scikit</li> <li> Mild R review</li> <li> Neural network review, and how to use dimensionality reduction and gradient descent to better ML models</li> </ul> </li> </ul>"}, {"location": "obsidian/portfolio%20cleanup/", "title": "Portfolio cleanup", "text": ""}, {"location": "obsidian/portfolio%20cleanup/#portfolio-cleanup", "title": "Portfolio Cleanup", "text": "<p>Like dusting off books on a shelf, there is knowledge waiting to be shared</p>"}, {"location": "obsidian/portfolio%20cleanup/#mkdocs-and-obsidian-markdown", "title": "MKDocs and Obsidian Markdown", "text": "<p>I\u2019m using Mara Li\u2019s excellent Obsidian Publish plugin to push notes from my obsidian vault to a github pages repo. I had tried this before, but only in testing. Now it is time to take existing data science projects I have and push to this static site. This means I just ripped the pre-existing notes off the website and restarted! Spring cleaning feels great</p>"}, {"location": "obsidian/portfolio%20cleanup/#what-am-i-making", "title": "What am I making?", "text": "<p>The only real decision is structure, of which there are a couple things to note:</p> <ul> <li>Obsidian Notes (like this basic one!) can be pushed to an obsidian sub directory to act as an archive</li> <li>The ML and NLP portfolios can have their own folders, with home pages for each section to act as summaries built from what I have already written</li> <li>The homepage of the size can just be an introduction to me, and service as the about page.</li> </ul>"}, {"location": "obsidian/portfolio%20cleanup/#aesthetics", "title": "Aesthetics", "text": "<p>For now! There shouldn\u2019t be any!</p> <p>Note</p> <p>Although I realized now I would like to set up automatic backlinks\u2026 (and to test callouts)</p>"}]}