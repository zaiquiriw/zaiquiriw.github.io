{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Home", "text": ""}, {"location": "#quick-test", "title": "Quick Test", "text": "<p>What is the default MKDocs homepage\u2026</p>"}, {"location": "ML%20Work/Classification/", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#classification", "title": "Classification", "text": "<p>This time we will be using linear models in order to classify observations. Linear models like logistic regression and Naive Bayes work by finding the probability of a target variable given a predictor variable. This means we are predicting a class as opposed to a continuous value like in linear regression. These models are great for data with outliers and are easy to implement and interpret. Linear regression isn\u2019t very flexible however, and Naive Bayes makes a naive assumption that predictors are independent. </p>"}, {"location": "ML%20Work/Classification/#what-is-our-data", "title": "What is Our Data?", "text": "<p>The weather data we used in our quantitative didn\u2019t have a suitable categorical target field, so we are switching to income census data. The data has a great binary classification in the form of an IncomeClass attribute that only states whether a given person\u2019s income is below or above 50k. We have plenty of categories for each person, and continuous measurements like age and work hours.</p> <p>The census itself is from the year 1994, and spans various socieo-economic groups. We both trying to predict this income classification based on all of the data, as well as just get an understanding of some key predictors in the data.</p> <p>With IncomeClass as our target, lets analyze the data!</p>"}, {"location": "ML%20Work/Classification/#reading-the-data", "title": "Reading the Data", "text": "<p>The data is stored as two files, with rows just delimited by commas, so we read them in to one whole data frame, and label the headers manual using our source as a reference. It\u2019s worth noting that this data was extracted with the intention of creating a classification model, so the two files are meant to be training and test data, but we are going to re-distribute the data later. <pre><code>income_train &lt;- read.table(\"adult.data\", sep=\",\", header=FALSE)\nincome_test &lt;- read.table(\"adult.test\", sep=\",\", header=FALSE)\nincome &lt;- rbind(income_test, income_train)\ncolnames(income) &lt;- c(\"Age\", \"WorkClass\", \"Weight\", \"Education\", \"YearsEdu\", \"Marital-Status\", \"Job\", \"Relationship\", \"Race\", \"Sex\", \"CapitalGain\", \"CapitalLoss\", \"HoursWorked\", \"NativeCountry\", \"IncomeClass\")\n#Just to check to make sure it read properly\nstr(income)\n</code></pre> Now we want to turn the qualitative data into factors.</p> <p>Find all attributes of income that are non-numeric</p> <ul> <li>sapply() returns a logical object of every attribute run through the given function</li> <li>which() returns all of the true indices of a logical object</li> <li>income[,"}, {"location": "ML%20Work/Classification/#training-data-exploration", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#spliting-training-data", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#textual-measurements", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#text-analysis-conclusion", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#visual-analysis", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#visual-analysis-conclusion", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#classification-regression", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#logistic-regression", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#explanation", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#looking-at-p-values", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#probability-warning", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#initial-impressions", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#improving-the-model", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#conclusions", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#naive-bayes-model", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#issues-in-the-data", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#predictions", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#initial-conclusion", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#the-confusion-matrix", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#accuracy", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#sensitivity-specificity", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#kappa", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#roc-curves-and-auc", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#matthews-correlation-coefficient-mcc", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#strengths-and-weaknesses", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#summary-of-metrics", "title": "Classification", "text": ""}, {"location": "ML%20Work/Classification/#conclusion", "title": "Classification", "text": ""}, {"location": "ML%20Work/Regression/", "title": "Regression", "text": ""}, {"location": "ML%20Work/Regression/#regression", "title": "Regression", "text": "<p>Using the Hungary Dataset Weather in Szeged 2006-2016 Found on Kaggle. </p> <p>Our goal is to see if we can see how other weather factors, such as Wind Speed and Humidity, relate to the difference between Apparent Temperature and actual Temperature. Though we identify apparent temperature as a very good predictor of the difference, we do not use this in this assignment as we are interested in exploring more the other factors that influence the disparity. </p> <p>Linear Regression is one of many supervised models of Machine Learning that functions by finding a trend in given data, using one or more input parameters to find a line of best fit, though it is not always a straight line. As shown by the name, linear regression models assume that the relationship between relevant attributes is linear. The model will predict coefficients for the effect of each predictor. It has low variance due to its linear nature, but with such an assumption, it will also be very high bias.</p>"}, {"location": "ML%20Work/Regression/#data-exploration", "title": "Data Exploration", "text": "<p>First, we read the data in, then divide our data up into training and testing. We have to add a column for the data we are interested in learning about, however, it is simply the difference between two other columns.  <pre><code>df &lt;- read.csv(\"weatherHistory.csv\")\n#Here we'll add the data that we are interested in: difference in Apparent Temp and Temp.\ndf$Temperature.Diff &lt;- df$Temperature..C. - df$Apparent.Temperature..C.\n#We'll also convert some data to factors for ease.\ndf$Precip.Type &lt;- as.factor(df$Precip.Type)\ndf$Summary &lt;- as.factor(df$Summary)\nstr(df)\n#Now we'll divide into train and test.\nset.seed(8)\ntrainindex &lt;- sample(1:nrow(df),nrow(df)*.8,replace=FALSE)\ntrain &lt;- df[trainindex,]\ntest &lt;- df[-trainindex,]\n</code></pre></p> <p>Next, we want to explore our training data.  <pre><code>names(df)\ndim(df)\nhead(df)\ncolMeans(df[4:11])\n\n#Noticing the mean of 0 of df$Loud.Cover, lets check its sum in specific.\nsum(df$Loud.Cover)\n\n#Let's see if we have any NAs more generally, now.\ncolSums(is.na(df))\n#Okay, so we don't have any NAs.\n\n#Now, lets see how R would summarize this data. \nsummary(df)\n#We would also like to look at this particular aspect to see how the different values pan out.\nsummary(df$Summary)\n</code></pre>   One thing we notice is that there is an attribute labeled \u2018Loud Cover\u2019 that all values are 0 in. Therefore, this will be an aspect that we will ignore.</p> <p>However, in the summary, we can notice that there is a minimum value of 0 on Pressure, which has an average and max similar to each other. We can assume a 0 is a NA value here. </p> <p>The other values that are 0 we can\u2019t make assumptions on validity.</p> <p>If Wind Speed is 0, so will Wind Bearing, we can realize from looking through the data in passing.    Since there is no place in earth without wind, we can also state that these values aren\u2019t accurate. After we look a the data some more, we\u2019ll decide how we want to clean them up.</p> <p>We cannot come to a clear resolution on other attributes.</p> <p>We\u2019ll pull up some graphs to get a better idea of what we have to do, now. Yellow dots are null precipitation days, green is rain, and blue is snow. <pre><code>cor(df[4:7])\nboxplot(df$Temperature.Diff,main=\"Apparent Temperature\")\nboxplot(df$Humidity,main=\"Humidity\")\nboxplot(df$Wind.Speed..km.h.,main=\"Wind Speed\")\n#pairs(df[4:7],main=\"Temperature, Humidity, and Wind Correlations\")\nplot(df$Temperature.Diff,df$Wind.Speed..km.h.,pch=21,bg=c(\"yellow\",\"green\",\"blue\")[as.integer(df$Precip.Type)])\nplot(df$Temperature.Diff,df$Humidity,pch=21,bg=c(\"yellow\",\"green\",\"blue\")[as.integer(df$Precip.Type)])\nplot(df$Temperature.Diff, df$Temperature..C.,pch=21,bg=c(\"yellow\",\"green\",\"blue\")[as.integer(df$Precip.Type)])\n</code></pre>   We can notice that there are some outliers in humidity at the 0 line we\u2019ll want to sort out, as well.</p> <p>We also notice a sort of \u201cknife\u201d shape in the data when being compared, at around 10 degrees Celsius.   By and large, we have such a large amount of data, it\u2019s difficult to notice quick correlations, aside from wind speed and temperature when it is snowing/below freezing.</p> <p>That\u2019s why we have Machine Learning, we suppose, even if it implies that linear regression may not be the best fit for this data set.</p> <p>While we would like to predict the results without the base temperature, we can see that is is very clearly related and helpful.</p> <p>Now, we\u2019ll clean up the data according to what we found. We\u2019ll clean up only what is referenced, but we will delete what we are uncertain about, since we have such a large amount of data. <pre><code>df[,6:7][df[,6:7]==0] &lt;- NA\ndf[,13:13][df[,13:13]==0] &lt;- NA\ndf &lt;- na.omit(df)\nsummary(df)\n</code></pre></p> <p>Now we\u2019ll do some more graphs. <pre><code>cor(df[4:7])\nboxplot(df$Temperature.Diff,main=\"Apparent Temperature\")\nboxplot(df$Humidity,main=\"Humidity\")\nboxplot(df$Wind.Speed..km.h.,main=\"Wind Speed\")\n#pairs(df[4:7],main=\"Temperature, Humidity, and Wind Correlations\")\nplot(df$Temperature.Diff,df$Wind.Speed..km.h.,pch=21,bg=c(\"yellow\",\"green\",\"blue\")[as.integer(df$Precip.Type)])\nplot(df$Temperature.Diff,df$Humidity,pch=21,bg=c(\"yellow\",\"green\",\"blue\")[as.integer(df$Precip.Type)])\nplot(df$Temperature.Diff, df$Temperature..C.,pch=21,bg=c(\"yellow\",\"green\",\"blue\")[as.integer(df$Precip.Type)])\n</code></pre></p> <p>Before we move on to linear regression, we have one more question before we investigate disparities in the data. </p> <p>Since this project has multiple contributors, perhaps there are even more hidden NA\u2019s. </p> <p>Let\u2019s see what the data looks like if we remove data where there is no difference.  <pre><code>diffOnly &lt;- df\ndiffOnly[,3:4][diffOnly[,3:3]==diffOnly[,4:4]] &lt;- NA\ndiffOnly &lt;- na.omit(diffOnly)\nplot(diffOnly$Temperature.Diff,diffOnly$Wind.Speed..km.h.,pch=21,bg=c(\"yellow\",\"green\",\"blue\")[as.integer(df$Precip.Type)])\nplot(diffOnly$Temperature.Diff,diffOnly$Humidity,pch=21,bg=c(\"yellow\",\"green\",\"blue\")[as.integer(df$Precip.Type)])\n#These graphs show the overall descriptor of the weather. There are 27 options.\nplot(diffOnly$Temperature.Diff,diffOnly$Wind.Speed..km.h.,pch=21,bg=c(\"white\",\"aquamarine\",\"blue\",\"brown\",\"green\",\"yellow\",\"red\",\"cyan\",\"darkgray\",\"darkgreen\",\"magenta\",\"orange\",\"dodgerblue\",\"forestgreen\",\"gold\",\"sienna\",\"thistle\",\"violet\",\"springgreen\",\"slateblue\",\"wheat\",\"tomato\",\"yellowgreen\",\"tan\",\"lightblue\",\"hotpink\",\"darkred\")[as.integer(df$Summary)])\nplot(diffOnly$Temperature.Diff,diffOnly$Humidity,pch=21,bg=c(\"white\",\"aquamarine\",\"blue\",\"brown\",\"green\",\"yellow\",\"red\",\"cyan\",\"darkgray\",\"darkgreen\",\"magenta\",\"orange\",\"dodgerblue\",\"forestgreen\",\"gold\",\"sienna\",\"thistle\",\"violet\",\"springgreen\",\"slateblue\",\"wheat\",\"tomato\",\"yellowgreen\",\"tan\",\"lightblue\",\"hotpink\",\"darkred\")[as.integer(df$Summary)])\nplot(df$Temperature.Diff, df$Temperature..C.,pch=21,bg=c(\"white\",\"aquamarine\",\"blue\",\"brown\",\"green\",\"yellow\",\"red\",\"cyan\",\"darkgray\",\"darkgreen\",\"magenta\",\"orange\",\"dodgerblue\",\"forestgreen\",\"gold\",\"sienna\",\"thistle\",\"violet\",\"springgreen\",\"slateblue\",\"wheat\",\"tomato\",\"yellowgreen\",\"tan\",\"lightblue\",\"hotpink\",\"darkred\")[as.integer(df$Summary)])\n</code></pre>   This does not seem to have effected data trends very much.</p> <p>Looking at the data based on Summary does not help us much, but we can notice that the cloud of data that does not seem to have much trend is in the violet(18) and slateblue (20), or rather Mostly Cloudy and Partly Cloudy. There isn\u2019t much helpful we can do with that information at this time, however.</p> <p>Since that seemed to cause no changes, but may have helped clean it up a small amount, let\u2019s write it to df. We\u2019ll also clean up the train and test data again. <pre><code>df &lt;- diffOnly\ntrainindex &lt;- sample(1:nrow(df),nrow(df)*.8,replace=FALSE)\ntrain &lt;- df[trainindex,]\ntest &lt;- df[-trainindex,]\n</code></pre></p> <p>Now, we\u2019ll move on to the regression.</p>"}, {"location": "ML%20Work/Regression/#linear-regression-simple", "title": "Linear Regression: Simple", "text": "<p>Let\u2019s start with a linear regression model with one predictor, wind speed, and summarize it.  <pre><code>simplelinreg &lt;- lm(Temperature.Diff~Wind.Speed..km.h.,data=train)\nsummary(simplelinreg)\n</code></pre>   So, it\u2019s better than nothing it seems. The R^2 isn\u2019t great, but we can see that there\u2019s enough of a correlation to count. We could get a better reading by using the actual temperature, since those are very closely related, but one goal of this is learning to understand how the change in temperature works based on other factors. </p> <p>Lets plot the residual errors, and evaluate. <pre><code>par(mfrow=c(2,2))\nplot(simplelinreg)\n</code></pre>   We can see that the trends are fairly close to the given lines. They are in no way perfect, but they seem to get the gist. The most concerning piece seems to be Residuals vs Leverage. The given line implies that we do have outlier (y-axis) leverage (x-axis) values that may influence our trend line. It may be something such as an issue during a case of severe weather, or a broken device used in data collection at that time.</p> <p>As well, this is only the data from our simple regression. We will be able to see how other models compare at a later time.</p>"}, {"location": "ML%20Work/Regression/#linear-regression-multiple", "title": "Linear Regression: Multiple", "text": "<p>Let\u2019s up the complexity, now. We\u2019ll build a multiple linear regression model, and see if we can improve the accuracy. <pre><code>multlinreg &lt;- lm(Temperature.Diff~Humidity+Wind.Speed..km.h.+Precip.Type,data=train)\nsummary(multlinreg)\npar(mfrow=c(2,2))\nplot(multlinreg)\n</code></pre>   We understand from our data exploration that Humidity, Wind Speed, and Precipitation Type all relate to the data in different ways. We can find different trends depending on what we\u2019re looking at, so we can ask the model to reference all of that data when its processing now. When the precipitation type was rain, it didn\u2019t add much to figuring things out, but knowing that it was in the snow range was very helpful.</p> <p>It\u2019s doing better than our simple model, getting the R^2 up much more and a lower RSE. The Residuals vs Leverage chart looks like it has encountered some issues, however the two entirely separate sections does match up with some inconsistent trends that we noticed when we were graphing the attributes we planned on working with. The Residuals vs Fitted and Scale-Location graphs look comparatively stellar. Normal Q-Q is about the same.</p>"}, {"location": "ML%20Work/Regression/#linear-regression-combinations", "title": "Linear Regression: Combinations", "text": "<p>Now let\u2019s go a step even farther. We\u2019ll use a combination of predictors, interaction effects, and polynomial regression to see if we can get even more accurate.  <pre><code>combolinreg &lt;- lm(Temperature.Diff~poly(Humidity*Wind.Speed..km.h.)+Precip.Type+Summary,data=train)\nsummary(combolinreg)\npar(mfrow=c(2,2))\nplot(combolinreg)\n</code></pre>   Here, we added Summary as well as an interaction effect with precipitation. We made this decision based on the cloud of Partly Cloudy values that didn\u2019t seem to follow other data, and we can see that some specific Summary values were quite helpful in the result, and some were not.</p> <p>Overall, though, R^2 is up a bit more, and RSE is down. It\u2019s not a huge change, but it does help. Humidity and Wind Speed seemed to have some similar trends and attributes when we graphed them, and the type of weather is related to the type of precipitation, which is why we had those certain attributes marked as an interaction effect.</p> <p>The residuals are now very different from the other two models\u2019 results. It seems like the values are much more as intended, horizontal where they should be to indicate a good fit, though Q-Q seems to be the same. The outlying x and y observations also seem to be different than the ones the other models denoted. </p>"}, {"location": "ML%20Work/Regression/#evaluation", "title": "Evaluation", "text": "<p>With each new type, using more aspects of different Machine Learning model results, we were able to increase the model\u2019s ability to find lines within the data, which should help us when we predict our results. In general, the more ways we let the data interact, the better the resulting model seemed to be, so long as we did not do it blindly. Depending on how related certain attributes are, they need to be treated differently, since some attributes are a result of other attributes that may be included in the data. </p> <p>So, in the end, the combination of interaction effects and multiple regression provided the best trends. Simple regression did not seem to fit the data well at all in comparison to the other models. The combination data may have only been a little about +.02 better on R^2 than multiple regression, however it is a significant enough change to be useful in data prediction. </p>"}, {"location": "ML%20Work/Regression/#predictions", "title": "Predictions", "text": "<p>Using the three models, we will predict and evaluate using the metric correlation and MSE.  <pre><code>simplepred &lt;- predict(simplelinreg,newdata=test)\nsimplecor &lt;- cor(simplepred,test$Temperature.Diff)\nsimplemse &lt;- mean((simplepred-test$Temperature.Diff)^2)\nsimplermse &lt;- sqrt(simplemse)\nmultpred &lt;- predict(multlinreg,newdata=test)\nmultcor &lt;- cor(multpred,test$Temperature.Diff)\nmultmse &lt;- mean((multpred-test$Temperature.Diff)^2)\nmultrmse &lt;- sqrt(multmse)\ncombopred &lt;- predict(combolinreg,newdata=test)\ncombocor &lt;- cor(combopred,test$Temperature.Diff)\ncombomse &lt;- mean((combopred-test$Temperature.Diff)^2)\ncombormse &lt;- sqrt(combomse)\n#Output results\nprint(\"-------Simple Model-------\")\nprint(paste(\"Correlation: \", simplecor))\nprint(paste(\"MSE: \", simplemse))\nprint(paste(\"RMSE: \", simplermse))\nprint(\"-------Multiple Model-------\")\nprint(paste(\"Correlation: \", multcor))\nprint(paste(\"MSE: \", multmse))\nprint(paste(\"RMSE: \", multrmse))\nprint(\"-------Combo Model-------\")\nprint(paste(\"Correlation: \", combocor))\nprint(paste(\"MSE: \", combomse))\nprint(paste(\"RMSE: \", combormse))\n\n\nanova(simplelinreg,multlinreg,combolinreg)\n</code></pre>   Judging by these values, we can verify our evaluation that the combination linear regression model was the best model for our data. The low MSE (mean squared error) in comparison to the simple and multiple models means that the mistakes made were smaller than others. The RMSE (root MSE) says that we were off, on average, .939 degrees Celsius. While this isn\u2019t entirely accurate, the range of the value was around -5 degrees to +10 degrees, and if someone was predicting the weather the average person would likely be tolerant of a one degree difference. In that sense, the multiple regression would also be considered accurate enough to be helpful. The simple model is not terrible either, however the low correlation and high MSE do support the fact that there is much more room for improvement.</p> <p>The difference in temperature is not extremely related to the wind speed, as we attempted in that first model. While it is a factor, the apparent temperature is a multifaceted issue better represented by numerous other effects, such as Humidity, precipitation, etc. </p> <p>Our results were also very good considering we were purposely avoiding using one aspect of given data, and that there were disparities showing what may have been differences due to how different contributors to the data set initially reading data in different ways.</p> <p>In summary, we can extract a surprising amount of data about the disparity in temperature based on wind, humidity, precipitation, and even descriptors of the sky. The more we combine usage of different attributes, acknowledging how they interact and work together, the better a result we can get.</p>"}, {"location": "ML%20Work/RegressionSVM/", "title": "Regression with SVM", "text": ""}, {"location": "ML%20Work/RegressionSVM/#svm-regression", "title": "SVM Regression", "text": "<p>Support Vector Machines can divide data into classes by a hyperplane in multidimensional space. This line separates classes by finding minimum distance of margins between support vectors. Once we calculate support vectors for our model (given an input of slack in the margins optimized with validation data), we can then classify the data in relation to the margins on the hyperplane.</p> <p>In the case of Regression, we apply this logic to fit a line to the data (as opposed to divide the data). Classification minimizes the margins such that all examples on either side of the margins are assumed to be classified correctly. SVM Regression\u2019s minimization function will instead find a hyper plane that fits the data within a certain accuracy. Specifically, the support vectors have the largest amount of error (distance) from the hyperplane, and everything within those support vectors (the margin) is assumed correctly fitted. Thus the hyperplane fits the data like simple regression.</p> <p>We are going to apply this algorithm to a simple 1 target, 1 predictor data set, and get a nice visual demonstration of the hyperplane.</p>"}, {"location": "ML%20Work/RegressionSVM/#exploring-our-data", "title": "Exploring our Data", "text": "<p>Our data, found on Kaggle is a very detailed and large collection of samples of larval fish data. However, I\u2019m only interested in the temperature of the ocean water in relation to it\u2019s salinity. So lets read it in and trim it down to just the necessary columns and a handleable training size. After finishing the document, I realized I had to trim the file size for github, so note that the actual data on kaggle is much larger!</p> <p>Note: We are going to sample the data to a smaller size right now, knowing that the last slide for SVM warned this may be neccessary</p> <p>We think it is worth leaving depth in, sense this will probably be a good predictor of temperature, if not as easy to see visually.</p> <p>I\u2019m leaving in the 2 code chunks below as comments, but they are referring to data before I trimmed down the csv file to upload to github!</p> <pre><code># This takes a second\n# ocean_data &lt;- read.csv(\"bottle.csv\")\n</code></pre> <pre><code># Selecting the wanted columns: 5= T_degC, 6=Depthm, and 7=Salnty\n# df &lt;- ocean_data[c(5,6,7)]\n# Removing NAs\n# which_nas &lt;- apply(df, 1, function(X) any(is.na(X)))\n# nas &lt;- length(which(which_nas))\n# size &lt;- nrow(df)\n# ratio &lt;- nas/size\n# size &lt;- format(size, big.mark = \",\", scientific = FALSE)\n# nas &lt;- format(length(which(which_nas)), big.mark = \",\", scientific = FALSE)\n# sprintf(\"%%%.2f of the %s large dataset contains NA's. Removing %s\", ratio*100, size, nas)\n# df &lt;- na.omit(df)\n</code></pre> <p>864,863 rows! That\u2019s still quite large! Lets reduce to exactly 10,000</p> <pre><code># set.seed(8)\n# df &lt;- df[sample(1:nrow(df),10000,replace=FALSE),]\n# nrow(df)\n# head(df)\n</code></pre> <pre><code># write.csv(df,\"ocean_data.csv\")\n</code></pre> <p>We can then split into training, testing, and validation data</p> <pre><code># The line below reads in the modified csv\nset.seed(8)\ndf &lt;- read.csv(\"ocean_data.csv\")\nspec &lt;- c(train=.6, test=.2, validate=.2)\ni &lt;- sample(cut(1:nrow(df), nrow(df)*cumsum(c(0,spec)), labels=names(spec)))\ntrain &lt;- df[i==\"train\",]\ntest &lt;- df[i==\"test\",]\nvald &lt;- df[i==\"validate\",]\n</code></pre>"}, {"location": "ML%20Work/RegressionSVM/#graphical-and-text-analysis", "title": "Graphical and Text Analysis", "text": "<p>Lets look at the data we are using to build our models:</p> <pre><code>summary(train)\n</code></pre> <p>Some things to note:</p> <ol> <li>A min depth of 0 is odd, but makes sense as a surface reading</li> <li>The outliers don\u2019t seem that far, so this might be some good data to look at</li> <li>From the mean values we can gather our average case: At a depth of of 228     meters, the temperature was ~11 degrees Celsius (51.8 Fahrenheit), with a     Salinity of ~34 percent, which matches averages easily found online.</li> <li>There weren\u2019t that many data reads in extremely salty waters (&gt;36%)</li> </ol> <p>I want to graph the correlation from salinity to temperature, a 3d graph of depth, temperature, and salinity, etc.</p> <pre><code>library(ggplot2)\ntheme_set(theme_minimal())\nmid &lt;- mean(train$Depthm)\nggplot(train, aes(x=Salnty, y=T_degC)) + \n  geom_point(pch=19, aes(color = Depthm), size=1) +\n  geom_smooth(formula=\"y~x\", method=\"lm\", color=\"red\", linetype=2) +\n  labs(title=\"Salinity vs Temperature\", x=\"Salinity (Percentage)\", y\n       =\"Temperature (C)\") +\n  scale_color_gradient2(midpoint=mid, low = \"red\", mid = \"blue\", high = \"red\", space = \"lab\")\n\nggplot(train, aes(x=Depthm, y=T_degC)) + \n  geom_point(pch=19, aes(color = Salnty), size=1) +\n  geom_smooth(formula=\"y~x\", method=\"lm\", color=\"red\", linetype=2) +\n  labs(title=\"Depth vs Temperature\", x=\"Depth (Meters)\", y\n       =\"Temperature (C)\")\n</code></pre> <p>We can see by this scatter plot, along with the convenient smoothing line that there is definitely a trend or correlation in the data. The smoothing line shows what the result of a linear regression might be on the model, and it\u2019s confidence interval (indicated by the gray border on the line) indicates the model\u2026 holds some water.</p> <p>Both Depth and Salinity have a strong correlation, but it seems Depth might have an exponential decay like relationship with temperature. That makes me wonder how the best way to portray these relationships.</p> <pre><code>cor(df)\n</code></pre> <p>Looking at the correlation as well as our graphs, simple visual analysis tells us that both depth and salinity are useful predictors of temperature, and a linear regression model would be useful. However, the relationships are complex, and we may find that a SVM regression model with a polynomial kernel might yield a good result. This is because it would be able to mold together our 2 predictors.</p> <p>Lets have a multiple-predictor linear regression model to compare our SVM results to and then carry on.</p> <pre><code># Creating a lm using both predictors and graphing predictions\nlm &lt;- lm(T_degC ~ Salnty + Depthm, train)\npred &lt;- predict(lm, newdata=test)\npreddf &lt;- data.frame(temp_pred=pred, salinity=test$Salnty)\nsummary(lm)\ncor &lt;-cor(pred, test$T_degC)\nmse &lt;- mean((pred-test$T_degC)^2)\nsprintf(\"Correlation of our prediction: %s\", cor)\nsprintf(\"Mean Squared Error: %s\", mse)\n\nggplot(test, aes(x=Salnty, y=T_degC)) + \n  geom_point(pch=19, aes(color = Depthm), size=1) +\n  geom_point(pch=1, color=\"yellow\",data=preddf, aes(x=salinity, y=temp_pred), size=.1) +\n  labs(title=\"Salinity vs Temperature\", x=\"Salinity (Percentage)\", y\n       =\"Temperature (C)\") +\n  scale_color_gradient2(midpoint=mid, low = \"red\", mid = \"blue\", high = \"red\", space = \"lab\")\n</code></pre>"}, {"location": "ML%20Work/RegressionSVM/#performing-svm-regression", "title": "Performing SVM Regression", "text": "<p>Lets just create a model for each type, tune the hyperparameters, and analyze the results.</p>"}, {"location": "ML%20Work/RegressionSVM/#linear-kernel", "title": "Linear Kernel", "text": "<pre><code>library(e1071)\nsvmlin &lt;- svm(T_degC~Depthm+Salnty, data=train, kernel=\"linear\", cost=10, scale=TRUE)\nsummary(svmlin)\npred &lt;- predict(svmlin, newdata=test)\ncor_svmlin &lt;- cor(pred, test$T_degC)\nmse_svmlin &lt;- mean((pred - test$T_degC)^2)\nsprintf(\"Correlation of our prediction: %s\", cor_svmlin)\nsprintf(\"Mean Squared Error: %s\", mse_svmlin)\n</code></pre> <p>That is a little worse then our baseline, lets try and tune it using a smaller subsample</p> <pre><code># Just using the ranges used in examples\ntune_svmlin &lt;- tune(svm, T_degC ~ Depthm + Salnty, data = vald, kernel=\"linear\", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\ntune_svmlin$best.model\n</code></pre> <p>We ran it with 1000 rows, and got a cost value of 1, then ran it with 3000 and got 100, then ran it with 5000, and then decided to wait for the 10000 to finish and got.. well 100. This suggest I should increase the range, but also that the linear model isn\u2019t the best solution to our data, considering the cost had to be so high.</p> <pre><code>svmlin &lt;- svm(T_degC~Depthm+Salnty, data=train, kernel=\"linear\", cost=5, scale=TRUE)\npred &lt;- predict(svmlin, newdata=test)\ntuned_cor_svmlin &lt;- cor(pred, test$T_degC)\ntuned_mse_svmlin &lt;- mean((pred - test$T_degC)^2)\nsprintf(\"Correlation of our prediction: %s\", tuned_cor_svmlin)\nsprintf(\"Mean Squared Error: %s\", tuned_mse_svmlin)\n</code></pre> <p>The error was higher, and the correlation was lower so we didn\u2019t improve. Tuning also barely effected the results!</p>"}, {"location": "ML%20Work/RegressionSVM/#polynomial-kernel", "title": "Polynomial Kernel", "text": "<pre><code>svmpoly &lt;- svm(T_degC~Depthm+Salnty, data=train, kernel=\"polynomial\", cost=5, scale=TRUE)\nsummary(svmpoly)\npred &lt;- predict(svmpoly, newdata=test)\ncor_svmpoly &lt;- cor(pred, test$T_degC)\nmse_svmpoly &lt;- mean((pred - test$T_degC)^2)\nsprintf(\"Correlation of our prediction: %s\", cor_svmpoly)\nsprintf(\"Mean Squared Error: %s\", mse_svmpoly)\n</code></pre> <p>Well that didn\u2019t go well! MSE is even higher then the baseline, so I can try to tune</p> <pre><code># tune_svmpoly &lt;- tune(svm, T_degC ~ Depthm + Salnty, data = vald, kernel=\"polynomial\", ranges=list(cost=c(0.001, 0.01, 0.1, # 1, 5, 10, 100)))\n# tune_svmlin$best.model\n</code></pre> <p>This would take a while to run, but would simply keep trying to increase the cost. But then that produces a model that over fits the training data! It seems this simply isn\u2019t the best way to fit this data and is a bit worse then simple regression.</p>"}, {"location": "ML%20Work/RegressionSVM/#radial-kernel", "title": "Radial Kernel", "text": "<p>My only hope\u2026</p> <pre><code>svmrad &lt;- svm(T_degC~Depthm+Salnty, data=train, kernel=\"radial\", cost=5, gamma=.5, scale=TRUE)\nsummary(svmrad)\npred &lt;- predict(svmrad, newdata=test)\ncor_svmrad &lt;- cor(pred, test$T_degC)\nmse_svmrad &lt;- mean((pred - test$T_degC)^2)\nsprintf(\"Correlation of our prediction: %s\", cor_svmrad)\nsprintf(\"Mean Squared Error: %s\", mse_svmrad)\n</code></pre> <p>WOAH. What a great result! That is a good correlation, and what seems to be a pretty good MSE, and both are much better then our baseline linear regression. Lets try and tune</p> <pre><code>tune_svmrad &lt;- tune(svm, T_degC ~ Depthm + Salnty, data = vald, kernel=\"radial\", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma=c(0.5, 1, 2, 3, 4)))\ntune_svmrad$best.model\n</code></pre> <pre><code>svmrad &lt;- svm(T_degC~Depthm+Salnty, data=train, kernel=\"radial\", cost=10, gamma=1, scale=TRUE)\npred &lt;- predict(svmrad, newdata=test)\ncor_svmrad &lt;- cor(pred, test$T_degC)\nmse_svmrad &lt;- mean((pred - test$T_degC)^2)\nsprintf(\"Correlation of our prediction: %s\", cor_svmrad)\nsprintf(\"Mean Squared Error: %s\", mse_svmrad)\n</code></pre> <p>It made the results a bit marginally worse, which suggests over fitting of the data with the training data vs the validation data.</p> <p>Either way, good results! Lets graph it!</p> <pre><code>preddf2 &lt;- data.frame(temp_pred=pred, salinity=test$Salnty)\n\nggplot(test, aes(x=Salnty, y=T_degC)) + \n  geom_point(pch=19, aes(color = Depthm), size=1) +\n  geom_point(pch=3, color=\"green\",data=preddf2, aes(x=salinity, y=temp_pred), size=.2) +\n  geom_point(pch=3, color=\"yellow\",data=preddf, aes(x=salinity, y=temp_pred), size=.2) +\n  labs(title=\"Salinity vs Temperature\", x=\"Salinity (Percentage)\", y\n       =\"Temperature (C)\") +\n  scale_color_gradient2(midpoint=mid, low = \"red\", mid = \"blue\", high = \"red\", space = \"lab\")\n</code></pre> <p>The green points indicate our new Just a bit lower then our linear regression model at the start, which suggests less susceptibility to outliers.</p>"}, {"location": "ML%20Work/RegressionSVM/#analysis", "title": "Analysis", "text": "<p>I would love to visualize the radial kernel to get an idea for how it works on this data, but quoting StatQuest: \u201cBecause the Radial Kernel finds Support Vector Classifiers in infinite dimensions, it\u2019s not possible to visualize what it does\u201d. While I know that you can still get an approximation, I\u2019m going to go ahead and skip figuring out how to do that for now.</p> <p>The main topic to analyze here is why the radial kernel was better than the polynomial or the linear kernel. Each kernel represents a different method of transforming the data so that a hyperplane may divide the data, or in this case, fit a function.</p> <ul> <li> <p>The linear kernel is simple, it fits a hyperplane to the data</p> </li> <li> <p>The polynomial kernel transforms the data in such a way to mimic adding more     features to the data set, really just by mapping the input data to a     polynomial of a higher degree. By mapping values in a higher degree space,     say, to the second degree, what really is a circular data set classification     can now have a straight line drawn through it.</p> </li> <li> <p>The radial kernel compares the distance between every 2 values in the input     data, and scales the data by the value of it\u2019s distance. This mimics nearest     neighbor, where the model predicts every value with increasing weight     supplied to its neighbors. The kernel can then map the input to a higher     (infinite) dimensional space where it is easiest to fit a hyperplane that     best maximizes the margins of the model\u2026 it\u2019s not exactly easy to wrap a     brain around</p> </li> </ul> <p>Because we found in our initial analysis that the relationship between our two predictors and our target was a complicated combination of a a not-so-linear relationship and a perhaps-exponential relationship, it makes sense that a complicated model like SVM with a radial kernel would have been able to find better results</p> <p>To think of this in terms of the data, the radial kernel was best able to interpret how depth related to temperature compared to salinity. As depth increased, it got colder, and salinity mattered less. The interactions between the data were important in this data! So while linear regression was good at approximating the relationship, the radial kernel was probably able to better fit the data.</p>"}, {"location": "ML%20Work/RegressionSVM/#link-dump", "title": "Link Dump!", "text": "<p>https://stackoverflow.com/questions/13353213/gradient-of-n-colors-ranging-from-color-1-and-color-2</p> <p>https://statisticsglobe.com/sprintf-r-function-example</p> <p>https://statisticsglobe.com/display-large-numbers-separated-with-comma-in-r</p> <p>I referenced the book for a lot of ggplot2 stuff, as well as code on SVM.</p> <p>https://www.youtube.com/watch?v=OpPBhBQA7fE&amp;list=PLfe6IcA_dEWkcHFfBA6XSXW31H8t4XSbB&amp;index=16</p> <p>https://stackoverflow.com/questions/37329074/geom-smooth-and-exponential-fits</p> <p>https://machinelearningmastery.com/regression-metrics-for-machine-learning/</p> <p>https://www.youtube.com/watch?v=Qc5IyLW_hns</p>"}, {"location": "ML%20Work/data%20exploration/", "title": "Data exploration", "text": ""}, {"location": "ML%20Work/data%20exploration/#data-exploration", "title": "Data Exploration", "text": ""}, {"location": "ML%20Work/data%20exploration/#the-premise", "title": "The Premise", "text": "<p>\u201cIn class, we covered how to do data exploration with statistical functions in R. In this assignment, you recreate that functionality in C++ code. This will prepare us to write algorithms in C++ in future assignments\u201d</p> <p>For me this is both a review of C++, but also a review of what correlation is.</p>"}, {"location": "ML%20Work/data%20exploration/#notes", "title": "Notes", "text": "<ul> <li>I deliberate whether returning range as the min and max, or as the difference between the two. I eventually chose just returning a min and max.</li> <li>I can\u2019t get relative links to work at the moment, hope it\u2019s fine that it is linking to the file hosted on the main site</li> <li></li> </ul>"}, {"location": "ML%20Work/data%20exploration/#conclusion", "title": "Conclusion", "text": ""}, {"location": "ML%20Work/data%20exploration/#the-code", "title": "The Code", "text": "<pre><code>#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n#include &lt;fstream&gt;\n#include &lt;algorithm&gt;\n#include &lt;iterator&gt;\n#include &lt;cmath&gt;\n\nusing namespace std;\n\n// TODO: Convert double vectors to taking in (explicitly) any numeric value\n// Reference: - Iterators: https://www.geeksforgeeks.org/iterators-c-stl/\n//            - What get's passed into sort: https://cplusplus.com/reference/iterator/RandomAccessIterator/\n\nclass Explore\n{\npublic:\n    // Calculate the sum of the vector\n    double sum_vector(vector&lt;double&gt; vect)\n    {\n        double sum = 0;\n        for (int i = 0; i &lt; vect.size(); i++)\n        {\n            sum += vect[i];\n        }\n        return sum;\n    }\n\n    // Calculate the mean of a vector\n    double mean_vector(vector&lt;double&gt; vect)\n    {\n        double mean = sum_vector(vect) / vect.size();\n        return mean;\n    }\n\n    // Calculate the median of a vector\n    double median_vector(vector&lt;double&gt; vect)\n    {\n        double median;\n        // Use an iterator because it is probably better -internet\n        vector&lt;double&gt;::iterator it;\n        // Find the center if it is even or odd\n        sort(vect.begin(), vect.end());\n        if (vect.size() % 2 == 0) // If there is an even number of elements\n        {\n            it = vect.begin() + vect.size() / 2 - 1;\n            median = (*it + *(it + 1)) / 2;\n        }\n        else // if there is an odd number of elements\n        {\n            it = vect.begin() + vect.size() / 2;\n            median = *it;\n        }\n        return median;\n    }\n\n    // Calculate the range of a vector\n    vector&lt;double&gt; range_vector(vector&lt;double&gt; vect)\n    {\n        vector&lt;double&gt; range = {max_vector(vect), min_vector(vect)};\n        return range;\n    }\n\n    // Calculate the max of a vector (Just for range)\n    double max_vector(vector&lt;double&gt; vect)\n    {\n        double max;\n        vector&lt;double&gt;::iterator it;\n        sort(vect.begin(), vect.end());\n        it = vect.end() - 1;\n        max = *it;\n        return max;\n    }\n\n    // Calculate the min of a vector (Just for range) just with a loop\n    double min_vector(vector&lt;double&gt; vect)\n    {\n        double min;\n        vector&lt;double&gt;::iterator it;\n        sort(vect.begin(), vect.end());\n        it = vect.begin();\n        min = *it;\n        return min;\n    }\n\n    // Calculate the covariance of two vectors\n    // Cov(x,y) = E((x-x_mean)(y-y_mean)))/n-1\n    double covar_vector(vector&lt;double&gt; x, vector&lt;double&gt; y)\n    {\n        double sum = 0;\n        double mean_x = mean_vector(x);\n        double mean_y = mean_vector(y);\n        for (int i = 0; i &lt; x.size(); i++)\n        {\n            float x_i_diff = x[i] - mean_x;\n            float y_i_diff = y[i] - mean_y;\n            float y_times_x_diff = x_i_diff * y_i_diff;\n            // cout &lt;&lt; x_i_diff &lt;&lt; \" * \" &lt;&lt; y_i_diff &lt;&lt; \" = \" &lt;&lt; y_times_x_diff &lt;&lt; endl;\n            sum = sum + y_times_x_diff;\n        }\n        return sum / (x.size() - 1);\n    }\n\n    // Calculate the correlation of two vectors\n    // Cor(x,y) = Cov(x,y)/(standard_devation(x)*standard_devation(y))\n    // Using the hint from the assignment:\n    // \"sigma of a vector can be calculated as the square root of variance(v,v)\"\n    double cor_vector(vector&lt;double&gt; x, vector&lt;double&gt; y)\n    {\n        double covar = covar_vector(x, y);\n        double sigma_x = sqrt(covar_vector(x, x));\n        double sigma_y = sqrt(covar_vector(y, y));\n        return covar / (sigma_x * sigma_y);\n    }\n\n    // Run suite of statistcal functions on a vector\n    void print_stats(vector&lt;double&gt; vect)\n    {\n        cout &lt;&lt; \"Sum:    \" &lt;&lt; sum_vector(vect) &lt;&lt; endl;\n        cout &lt;&lt; \"Mean:   \" &lt;&lt; mean_vector(vect) &lt;&lt; endl;\n        cout &lt;&lt; \"Median: \" &lt;&lt; median_vector(vect) &lt;&lt; endl;\n        vector&lt;double&gt; range = range_vector(vect);\n        cout &lt;&lt; \"Range:  \" &lt;&lt; range[1] &lt;&lt; \", \" &lt;&lt; range[0] &lt;&lt; endl;\n    }\n};\n\nint main(int argc, char **argv)\n{\n    ifstream inFS;\n    string line;\n    string rm_in, medv_in;\n    const int MAX_LEN = 1000;\n    vector&lt;double&gt; rm(MAX_LEN), medv(MAX_LEN);\n\n    cout &lt;&lt; \"Opening file Boston.csv.\" &lt;&lt; endl;\n\n    inFS.open(\"Boston.csv\");\n    if (!inFS.is_open())\n    {\n        cout &lt;&lt; \"Error opening file Boston.csv.\" &lt;&lt; endl;\n        return 1;\n    }\n\n    cout &lt;&lt; \"Reading line 1 of Boston.csv.\" &lt;&lt; endl;\n    getline(inFS, line);\n\n    // echo heading\n    cout &lt;&lt; \"Headings: \" &lt;&lt; line &lt;&lt; endl;\n\n    // read data\n    int numObservations = 0;\n    while (inFS.good())\n    {\n        getline(inFS, rm_in, ',');\n        getline(inFS, medv_in, '\\n');\n        rm.at(numObservations) = stof(rm_in);\n        medv.at(numObservations) = stof(medv_in);\n\n        numObservations++;\n    }\n\n    rm.resize(numObservations);\n    medv.resize(numObservations);\n\n    cout &lt;&lt; \"New Length: \" &lt;&lt; rm.size() &lt;&lt; endl;\n\n    cout &lt;&lt; \"Closing file Boston.csv.\" &lt;&lt; endl;\n    inFS.close(); // Done\n\n    cout &lt;&lt; \"Number of records: \" &lt;&lt; numObservations &lt;&lt; endl;\n\n    // Create an Explore object to use stats functions\n    Explore explore;\n\n    cout &lt;&lt; \"\\nStats for rm\" &lt;&lt; endl;\n    explore.print_stats(rm);\n\n    cout &lt;&lt; \"\\nStats for medv\" &lt;&lt; endl;\n    explore.print_stats(medv);\n\n    cout &lt;&lt; \"\\n Covariance = \" &lt;&lt; explore.covar_vector(rm, medv) &lt;&lt; endl;\n\n    cout &lt;&lt; \"\\n Correlation = \" &lt;&lt; explore.cor_vector(rm, medv) &lt;&lt; endl;\n\n    cout &lt;&lt; \"\\nProgram terminated.\" &lt;&lt; endl;\n}\n</code></pre>"}, {"location": "ML%20Work/data%20exploration/#returns", "title": "Returns", "text": "<pre><code>Opening file Boston.csv.\nReading line 1 of Boston.csv.\nHeadings: rm,medv\nNew Length: 506\nClosing file Boston.csv.\nNumber of records: 506\n\nStats for rm\nSum:    3180.03\nMean:   6.28463\nMedian: 6.2085\nRange:  3.561, 8.78\n\nStats for medv\nSum:    11401.6\nMean:   22.5328\nMedian: 21.2\nRange:  5, 50\n\n Covariance = 4.49345\n\n Correlation = 0.69536\n\nProgram terminated.\n</code></pre>"}, {"location": "ML%20Work/data%20exploration/#built-into-r-or-c", "title": "Built into R or C++", "text": "<p>I believe that it was clearly easier to use R functions vs going through and making these functions in C++. This indicates the value of use using R into the future for machine learning. A more straight forward way to analyze data will allow us to understand our data models.</p> <p>I will note that someone fluent in C++ would do better than I did, considering I was just stumbling around for a bit trying to remember how import a library for a second there.</p>"}, {"location": "ML%20Work/data%20exploration/#statistical-value", "title": "Statistical Value", "text": "<p>What statistical measures did I evaluate:</p> <ul> <li>Mean: A mean is an average of the data set, and represents the typical or most likely value from a dataset. Knowing what values in a dataset tend to be is important in understanding what general trend of all the data in your dataset is. You can compare values to this to find outliers and such.</li> <li>Median: The center value of the data as it is in sorted order. This tells you a central tendency independent of skewed data or large outliers</li> <li>Range: is the minimum and maximum values the values of the dataset might take. It is useful to understand how a dataset is bounded, both to see how far outliers might be from the center of a dataset, and just to get an idea for what the data looks like in scale.</li> </ul> <p>Whenever we are organizing data for a machine learning algorithm, we must understand our data ourselves to have a hope of predicting a trend given our data. Looking at values like mean or median tell us easy to understand generalizations about data so that we may get a general understanding of the meaning of our data without having to analyze every data point. Given more and more powerful generalization tools, or methods of analyzation, we can grow more and more confident in the understanding of our data.</p> <p>If we can see a general trend using our descriptive statistical measurements, then we can assure our model will be able to eventually get the specific trend data we hope to predict from that dataset.</p>"}, {"location": "ML%20Work/data%20exploration/#covariance-and-correlation", "title": "Covariance and Correlation", "text": "<p>Given two attributes that may or may not be related, we may find the covariance and correlation between those two bits of data. The covariance tells how one attributes data might relate to another. If x\u2019s covariance to y is a high positive number, we know that as x goes up y goes down, and vice versa. Correlation is just a version of that number, scaled down to a range of (-1, 1) in order to make the factor much more uniform</p> <p>The values are different from those above, considering they are not just measurements of an attribute of data but an extrapolation from that data about relationships or patterns. This is very useful when working in ml, as our end goal is figuring out how data tends to relate to certain results. Using how data correlates then directly supports our end goal of predicting outcomes, or just understanding complex relationships.</p>"}, {"location": "ML%20Work/ml%20overview/", "title": "ML Work", "text": ""}, {"location": "ML%20Work/ml%20overview/#machine-learning-work", "title": "Machine Learning Work", "text": "<p>Back in 2022 I took an introduction to Machine Learning with the wonderful Karen Mazidi who gave us a large overview of both data science basics, and basic machine learning. The class was project based, with a focus on providing documentation of the process.</p>"}, {"location": "ML%20Work/ml%20overview/#my-projects", "title": "My Projects", "text": "<p>While I will leave my original code open source on github the following markdown documents are reformatted for easier consumption</p>"}, {"location": "ML%20Work/ml%20overview/#data-exploration-in-c", "title": "Data Exploration in C++", "text": "<p>This project is a reminder that I must understand these algorithms on the low level, so we both reviewed the statistical basics and implementing covariance and correlation calculation in C++.</p>"}, {"location": "ML%20Work/ml%20overview/#linear-models", "title": "Linear Models", "text": ""}, {"location": "ML%20Work/ml%20overview/#logistic-regression-and-naive-bayes", "title": "Logistic Regression and Naive Bayes", "text": ""}, {"location": "ML%20Work/ml%20overview/#similarities", "title": "Similarities", "text": ""}, {"location": "ML%20Work/ml%20overview/#svm-and-ensemble-study", "title": "SVM and Ensemble Study", "text": ""}, {"location": "ML%20Work/ml%20overview/#sklearn", "title": "Sklearn", "text": ""}, {"location": "ML%20Work/ml%20overview/#image-classification-in-keras", "title": "Image Classification in Keras", "text": ""}, {"location": "ML%20Work/ml%20overview/#learning-in-r", "title": "Learning in R", "text": "<p>We covered using R for a variety of algorithms:</p> <ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Naive Bayes</li> <li>kNN</li> <li>k-means Clustering</li> <li>Decision Trees and Random Forests</li> <li>Support Vector Machines</li> </ul> <p>As well as best practices for picking good attributes for analysis, cleaning up data in CSVs, and visualizing the results</p>"}, {"location": "ML%20Work/ml%20overview/#learning-in-python", "title": "Learning in Python", "text": "<p>We then pivoted to implementing these solutions in the Python package ecosystem, using:</p> <ul> <li>NumPy</li> <li>Pandas</li> <li>Scikit-Learn</li> <li>Seaborn</li> </ul> <p>To implement all the algorithms we just learned. Python allowed us to branch into Neural Networks using Keras for the implementation of zero-shot classification</p> <p>Note</p> <p>We even touched on Hidden Markov Models and Bayesian nets, but not much on their implementation</p>"}, {"location": "ML%20Work/ml%20overview/#continued-work", "title": "Continued Work", "text": "<p>This class carried into my work in NLP, which happened to coincide with the emergence of Open-AI\u2019s ChatGPT. Check it out for work on more advanced neural nets</p>"}, {"location": "obsidian/copart%20internship/", "title": "Copart internship", "text": ""}, {"location": "obsidian/copart%20internship/#data-science-at-copart", "title": "Data Science at Copart", "text": "<p>The company Copart is right next to my house. Right now, as of 2 days ago, they have a position open for a data scientist internship right now! I meet all of the requirements, and have the portfolio to support it. Now I just need to put all that together for a</p>"}, {"location": "obsidian/copart%20internship/#the-job", "title": "The Job", "text": "<p>Copart is looking for a data scientist who will work closely with IT and various other departments to drive insight into data and deliver machine learning solutions to improve Copart\u2019s operations. This data scientist will also design/manipulate large scale data sets from a multitude of sources, work to operationalize and integrate machine learning solutions into Copart\u2019s current products and visualize and report on findings and results to provide insight to the organization.  </p> <p>Job Duties</p> <ul> <li>Develop new predictive models using advanced techniques</li> <li>Apply critical thinking to ensure data integrity and quality control is applied to each dataset, model and other analysis prior to presenting with internal customers</li> <li>Coordinate with different functional teams to operationalize, and monitor machine learning solutions</li> <li>Apply statistical methodologies such as cluster and regression analysis, if necessary.</li> <li>Act as a proponent of data science/analytics to senior leadership and others by being able to explain the benefits of machine learning, and other techniques.</li> <li>6 Months of experience (relevant academic internships &amp; projects can be considered in lieu of professional experience) with machine learning, statistical modeling, and data mining techniques.</li> <li>Bachelor or Master\u2019s degree in highly quantitative field (computer science, mathematics, machine learning, statistics) or equivalent experience</li> <li>Proficiency in either R or Python</li> <li>Proficiency in data sourcing/manipulation in SQL</li> <li>Bachelor or Master\u2019s degree in highly quantitative field (computer science, mathematics, machine learning, statistics) or equivalent experience</li> <li>Experience applying various machine learning techniques, specifically neural networks and gradient boosted machines, and understanding the key parameters that affect their performance</li> <li>Strong data visualization skills using open source tools (plotly, ggplot2, shiny)</li> <li>Experience with both supervised and unsupervised modeling techniques</li> </ul> <p>This is, simply, exactly what I have experience in. But I haven\u2019t done anything in the field recently.</p>"}, {"location": "obsidian/copart%20internship/#what-i-want-to-do", "title": "What I Want to Do", "text": "<p>I am terrified of applying to jobs. So I am going to compromise between not applying and applying right now. I\u2019d say I have to prep my past work on the subject</p> <ul> <li> Combine my work from Mazidi\u2019s NLP and ML classes into one portfolio in markdown</li> <li> Host it on my URL with a home page summary that links to my linkedin and github</li> <li> Clean up my github</li> <li> Clean up my Resume to be focused on Data Science</li> <li> Review focus points on the list of requirements:<ul> <li> Python basic programming questions</li> <li> Profeciency with ggplot2 and pandas, matplot, pytorch, and scikit</li> <li> Mild R review</li> <li> Neural network review, and how to use dimensionality reduction and gradient descent to better ML models</li> </ul> </li> </ul>"}, {"location": "obsidian/portfolio%20cleanup/", "title": "Portfolio cleanup", "text": ""}, {"location": "obsidian/portfolio%20cleanup/#portfolio-cleanup", "title": "Portfolio Cleanup", "text": "<p>Like dusting off books on a shelf, there is knowledge waiting to be shared</p>"}, {"location": "obsidian/portfolio%20cleanup/#mkdocs-and-obsidian-markdown", "title": "MKDocs and Obsidian Markdown", "text": "<p>I\u2019m using Mara Li\u2019s excellent Obsidian Publish plugin to push notes from my obsidian vault to a github pages repo. I had tried this before, but only in testing. Now it is time to take existing data science projects I have and push to this static site. This means I just ripped the pre-existing notes off the website and restarted! Spring cleaning feels great</p>"}, {"location": "obsidian/portfolio%20cleanup/#what-am-i-making", "title": "What am I making?", "text": "<p>The only real decision is structure, of which there are a couple things to note:</p> <ul> <li>Obsidian Notes (like this basic one!) can be pushed to an obsidian sub directory to act as an archive</li> <li>The ML and NLP portfolios can have their own folders, with home pages for each section to act as summaries built from what I have already written</li> <li>The homepage of the size can just be an introduction to me, and service as the about page.</li> </ul>"}, {"location": "obsidian/portfolio%20cleanup/#aesthetics", "title": "Aesthetics", "text": "<p>For now! There shouldn\u2019t be any!</p> <p>Note</p> <p>Although I realized now I would like to set up automatic backlinks\u2026 (and to test callouts)</p>"}]}